{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from ogb.lsc.pcqm4m_dgl import DglPCQM4MDataset\r\n",
    "dataset=DglPCQM4MDataset(root='./')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore Datset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "print('Total length:',len(dataset))\r\n",
    "splitted_idx = dataset.get_idx_split()\r\n",
    "print(dict((k,len(v)) for k,v in splitted_idx.items()))\r\n",
    "print()\r\n",
    "\r\n",
    "first_s, first_t = dataset[0]\r\n",
    "max_t = first_t.numpy()\r\n",
    "min_t = first_t.numpy()\r\n",
    "max_nodes = first_s.num_nodes()\r\n",
    "min_nodes = max_nodes\r\n",
    "n_maximum = first_s.ndata['feat'].numpy().max(0)\r\n",
    "n_minimum = n_maximum.copy()\r\n",
    "e_maximum = first_s.edata['feat'].numpy().max(0)\r\n",
    "e_minimum = e_maximum.copy()\r\n",
    "\r\n",
    "for i, (g,t) in tqdm(enumerate(dataset)):\r\n",
    "    t_np = t.numpy()\r\n",
    "    if not np.any(np.isnan(t_np)):\r\n",
    "        max_t = np.maximum(max_t, t_np)\r\n",
    "        min_t = np.minimum(min_t, t_np)\r\n",
    "    \r\n",
    "    nn = g.num_nodes()\r\n",
    "    max_nodes = np.maximum(max_nodes, nn)\r\n",
    "    min_nodes = np.minimum(min_nodes, nn)\r\n",
    "\r\n",
    "    n_feat = g.ndata['feat'].numpy()\r\n",
    "    n_maximum = np.maximum(n_maximum, n_feat.max(0))\r\n",
    "    n_minimum = np.minimum(n_minimum, n_feat.min(0))\r\n",
    "    \r\n",
    "    e_feat = g.edata['feat'].numpy()\r\n",
    "    if e_feat.shape[0] > 0:\r\n",
    "        e_maximum = np.maximum(e_maximum, e_feat.max(0))\r\n",
    "        e_minimum = np.minimum(e_minimum, e_feat.min(0))\r\n",
    "\r\n",
    "print()\r\n",
    "print(f'max_t = {max_t}')\r\n",
    "print(f'min_t = {min_t}')\r\n",
    "\r\n",
    "print()\r\n",
    "print(f'max_nodes = {max_nodes}')\r\n",
    "print(f'min_nodes = {min_nodes}')\r\n",
    "\r\n",
    "print()\r\n",
    "print(f'n_maximum = {n_maximum}')\r\n",
    "print(f'n_minimum = {n_minimum}')\r\n",
    "\r\n",
    "print()\r\n",
    "print(f'e_maximum = {e_maximum}')\r\n",
    "print(f'e_minimum = {e_minimum}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total length: 3803453\n",
      "{'train': 3045360, 'valid': 380670, 'test': 377423}\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3803453it [07:22, 8596.61it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "max_t = 47.02399444580078\n",
      "min_t = 0.3755171000957489\n",
      "\n",
      "max_nodes = 51\n",
      "min_nodes = 1\n",
      "\n",
      "n_maximum = [52  2  6  9  4  4  5  1  1]\n",
      "n_minimum = [0 0 0 4 0 0 0 0 0]\n",
      "\n",
      "e_maximum = [3 2 1]\n",
      "e_minimum = [0 0 0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save HDF5 Parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tqdm\r\n",
    "import torch\r\n",
    "import h5py\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "DATASET_NAME = 'PCQM4M'\r\n",
    "trainset, valset, testset = dataset[splitted_idx['train']], dataset[splitted_idx['valid']], dataset[splitted_idx['test']]\r\n",
    "\r\n",
    "iter_tracker = tqdm.tqdm\r\n",
    "int_type = 'uint8'\r\n",
    "\r\n",
    "def save_dataset(ds, data, start, chunk_size):\r\n",
    "    for i in iter_tracker(range(start, min(start+chunk_size, len(data)))):\r\n",
    "        g, t = data[i]\r\n",
    "        grp = ds.create_group(f'{i:0>10d}')\r\n",
    "        dgrp = grp.create_group('data')\r\n",
    "        \r\n",
    "        dgrp.attrs['num_nodes'] = g.num_nodes()\r\n",
    "        dgrp.attrs['num_edges'] = g.num_edges()\r\n",
    "        \r\n",
    "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy().astype(int_type)\r\n",
    "        \r\n",
    "        dnfgrp = dgrp.create_group('features/nodes')\r\n",
    "        defgrp = dgrp.create_group('features/edges')\r\n",
    "        for fname, fval in g.ndata.items():\r\n",
    "            dnfgrp[fname] = fval.numpy().reshape([-1,9]).astype(int_type)\r\n",
    "        for fname, fval in g.edata.items():\r\n",
    "            defgrp[fname] = fval.numpy().reshape([-1,3]).astype(int_type)\r\n",
    "        \r\n",
    "        tgrp = grp.create_group('targets')\r\n",
    "        t = t.numpy()\r\n",
    "        t[np.isnan(t)] = -10.\r\n",
    "        tgrp['value'] = t\r\n",
    "        if not (i%10000):\r\n",
    "            ds.file.flush()\r\n",
    "\r\n",
    "\r\n",
    "chunk_size = 400000\r\n",
    "\r\n",
    "def save_data_chunks(split, dat):\r\n",
    "    for i in range(0, len(dat), chunk_size):\r\n",
    "        part_no = i//chunk_size\r\n",
    "        dest_file = f'datasets/{DATASET_NAME}/parts/{DATASET_NAME}_{split}_{part_no:0>3d}.h5'\r\n",
    "        Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\r\n",
    "        \r\n",
    "        print(f'Saving to {dest_file}...')\r\n",
    "\r\n",
    "        with h5py.File(dest_file, 'w') as file:\r\n",
    "            ds = file.create_group(f'{DATASET_NAME}')\r\n",
    "            ds.attrs['num_atom_type'] = (n_maximum+1)\r\n",
    "            ds.attrs['num_bond_type'] = (e_maximum+1)\r\n",
    "            ds.attrs['num_atom_feats'] = n_maximum.shape[0]\r\n",
    "            ds.attrs['num_bond_feats'] = e_maximum.shape[0]\r\n",
    "            ds.attrs['num_min_atoms'] = min_nodes\r\n",
    "            ds.attrs['num_max_atoms'] = max_nodes\r\n",
    "            ds.attrs['min_targ_val'] = min_t\r\n",
    "            ds.attrs['max_targ_val'] = max_t\r\n",
    "\r\n",
    "            part_ds = ds.create_group(f'{split}_{part_no:0>3d}')\r\n",
    "            save_dataset(part_ds, dat, i, chunk_size)\r\n",
    "\r\n",
    "\r\n",
    "for s, ds in zip(['training', 'validation', 'test'],\r\n",
    "                 [trainset, valset, testset]):\r\n",
    "    save_data_chunks(s, ds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Parent Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import h5py as h5\r\n",
    "from pathlib import Path\r\n",
    "from collections import defaultdict\r\n",
    "\r\n",
    "DATASET_NAME = 'PCQM4M'\r\n",
    "parts_p = Path(f'datasets/{DATASET_NAME}/parts')\r\n",
    "\r\n",
    "groups = defaultdict(list)\r\n",
    "for fl in parts_p.rglob('*.h5'):\r\n",
    "    with h5.File(str(fl),'r') as file:\r\n",
    "        for name, grp in file[DATASET_NAME].items():\r\n",
    "            gg = name.split('_')[0]\r\n",
    "            print(name, gg)\r\n",
    "            groups[gg].append( (name, fl.relative_to(fl.parents[1]).as_posix(), grp.name) )\r\n",
    "            \r\n",
    "with h5.File(f'datasets/{DATASET_NAME}/{DATASET_NAME}.h5','w') as file:\r\n",
    "    ds = file.create_group(f'{DATASET_NAME}')\r\n",
    "    ds.attrs['num_atom_type'] = (n_maximum+1)\r\n",
    "    ds.attrs['num_bond_type'] = (e_maximum+1)\r\n",
    "    ds.attrs['num_atom_feats'] = n_maximum.shape[0]\r\n",
    "    ds.attrs['num_bond_feats'] = e_maximum.shape[0]\r\n",
    "    ds.attrs['num_min_atoms'] = min_nodes\r\n",
    "    ds.attrs['num_max_atoms'] = max_nodes\r\n",
    "    ds.attrs['min_targ_val'] = min_t\r\n",
    "    ds.attrs['max_targ_val'] = max_t\r\n",
    "    \r\n",
    "    for grp_name, grp_links in groups.items():\r\n",
    "        grp_ds = ds.create_group(grp_name)\r\n",
    "        for chunk_name, fl_link, item_link in grp_links:\r\n",
    "            grp_ds[chunk_name] = h5.ExternalLink(fl_link, item_link)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_000 test\n",
      "training_000 training\n",
      "training_001 training\n",
      "training_002 training\n",
      "training_003 training\n",
      "training_004 training\n",
      "training_005 training\n",
      "training_006 training\n",
      "training_007 training\n",
      "validation_000 validation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "with h5.File(f'datasets/{DATASET_NAME}/{DATASET_NAME}.h5','r') as file:\r\n",
    "    ds = file[f'{DATASET_NAME}']\r\n",
    "    print(len(ds['training/training_007']))\r\n",
    "    print(len(ds['validation']))\r\n",
    "    print(len(ds['test']))\r\n",
    "    print(ds[f'test/test_000/{300000:0>10d}/targets/value'][()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "245360\n",
      "1\n",
      "1\n",
      "-10.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
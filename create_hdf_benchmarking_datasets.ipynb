{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g-JTMJcMHzg"
      },
      "source": [
        "# Clone Repository and Download Datasets from Dwivedi et al."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqjU1dDCMDFK",
        "outputId": "11b4e95c-61bd-4599-c0db-7c3e3bf096fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'benchmarking-gnns'...\n",
            "remote: Enumerating objects: 882, done.\u001b[K\n",
            "remote: Total 882 (delta 0), reused 0 (delta 0), pack-reused 882\u001b[K\n",
            "Receiving objects: 100% (882/882), 2.65 MiB | 1.81 MiB/s, done.\n",
            "Resolving deltas: 100% (677/677), done.\n",
            "/home/ml4ig1/Documents code/egt/benchmarking-gnns/data\n",
            "\n",
            "downloading ZINC.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    53    0    53    0     0    188      0 --:--:-- --:--:-- --:--:--   188\n",
            "100   320  100   320    0     0    450      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 56.1M  100 56.1M    0     0  17.7M      0  0:00:03  0:00:03 --:--:-- 29.7M\n",
            "\n",
            "downloading ZINC-full.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    58    0    58    0     0    214      0 --:--:-- --:--:-- --:--:--   214\n",
            "100   320  100   320    0     0    463      0 --:--:-- --:--:-- --:--:--   463\n",
            "100 1162M  100 1162M    0     0  21.5M      0  0:00:53  0:00:53 --:--:-- 23.2M\n",
            "\n",
            "downloading MNIST.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    54    0    54    0     0    182      0 --:--:-- --:--:-- --:--:--   181\n",
            "100   320  100   320    0     0    455      0 --:--:-- --:--:-- --:--:--   455\n",
            "100 1320M  100 1320M    0     0  22.1M      0  0:00:59  0:00:59 --:--:-- 23.6MM\n",
            "\n",
            "downloading CIFAR10.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    56    0    56    0     0    130      0 --:--:-- --:--:-- --:--:--   130\n",
            "100   320  100   320    0     0    373      0 --:--:-- --:--:-- --:--:--   373\n",
            "100 2394M  100 2394M    0     0  27.5M      0  0:01:27  0:01:27 --:--:-- 26.4M2174M    0     0  27.7M      0  0:01:26  0:01:18  0:00:08 29.7M\n",
            "\n",
            "downloading SBM_CLUSTER.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    60    0    60    0     0    186      0 --:--:-- --:--:-- --:--:--   186\n",
            "100   320  100   320    0     0    448      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1203M  100 1203M    0     0  26.8M      0  0:00:44  0:00:44 --:--:-- 25.3M\n",
            "\n",
            "downloading SBM_PATTERN.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    60    0    60    0     0    223      0 --:--:-- --:--:-- --:--:--   223\n",
            "100   320  100   320    0     0    513      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1884M  100 1884M    0     0  29.4M      0  0:01:03  0:01:03 --:--:-- 31.0M\n",
            "\n",
            "downloading TSP.pkl...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    52    0    52    0     0    196      0 --:--:-- --:--:-- --:--:--   196\n",
            "100   320  100   320    0     0    441      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1781M  100 1781M    0     0  26.9M      0  0:01:06  0:01:06 --:--:-- 26.5M\n",
            "\n",
            "downloading CSL.zip...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    52    0    52    0     0    174      0 --:--:-- --:--:-- --:--:--   174\n",
            "100   320  100   320    0     0    446      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 27407  100 27407    0     0  25880      0  0:00:01  0:00:01 --:--:-- 25880\n",
            "Archive:  CSL.zip\n",
            "  inflating: ./graphs_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./__MACOSX/._graphs_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./X_eye_list_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./__MACOSX/._X_eye_list_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./X_unity_list_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./__MACOSX/._X_unity_list_Kary_Deterministic_Graphs.pkl  \n",
            "  inflating: ./y_Kary_Deterministic_Graphs.pt  \n",
            "  inflating: ./__MACOSX/._y_Kary_Deterministic_Graphs.pt  \n",
            "/home/ml4ig1/Documents code/egt/benchmarking-gnns\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shamim-hussain/benchmarking-gnns.git\n",
        "%cd benchmarking-gnns/data/\n",
        "!bash script_download_all_datasets.sh\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrzCtDPZRm7w"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJryCgq_iuk"
      },
      "source": [
        "# Convert Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ogb\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.0.0+cu117'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from packaging import version\n",
        "version.parse(torch.__version__) < version.parse(\"1.13.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 pyg=2.4.0 ogb=1.3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from ogb.linkproppred import PygLinkPropPredDataset\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'split_idx' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dataset \u001b[39m=\u001b[39m PygLinkPropPredDataset(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mogbl-ppa\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                                 transform\u001b[39m=\u001b[39mT\u001b[39m.\u001b[39mToSparseTensor())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#split_idx = dataset.get_idx_split() \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(dataset[split_idx[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]], batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m valid_loader \u001b[39m=\u001b[39m DataLoader(dataset[split_idx[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m]], batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(dataset[split_idx[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]], batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'split_idx' is not defined"
          ]
        }
      ],
      "source": [
        "# Download and process data at './dataset/ogbg_molhiv/'\n",
        "#dataset = PygLinkPropPredDataset(name='ogbl-ppa',\n",
        "#                                transform=T.ToSparseTensor())\n",
        "##split_idx = dataset.get_idx_split() \n",
        "#train_loader = DataLoader(dataset[split_idx['train']], batch_size=32, shuffle=True)\n",
        "#valid_loader = DataLoader(dataset[split_idx['valid']], batch_size=32, shuffle=False)\n",
        "#test_loader = DataLoader(dataset[split_idx['test']], batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and process data at './dataset/ogbg_molhiv/'\n",
        "dataset = PygLinkPropPredDataset(name='ogbl-ppa',\n",
        "                                transform=T.ToSparseTensor())\n",
        "#split_idx = dataset.get_idx_split() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = dataset[0]\n",
        "data.x = data.x.to(torch.float)\n",
        "#data.x = torch.cat([data.x, torch.load('embedding.pt')], dim=-1)\n",
        "data = data.to(device)\n",
        "\n",
        "split_edge = dataset.get_edge_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_edge = dataset.get_edge_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'set_diag'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49madj_t\u001b[39m.\u001b[39;49mset_diag()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'set_diag'"
          ]
        }
      ],
      "source": [
        "data.adj_t.set_diag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(crow_indices=tensor([       0,       11,       22,  ..., 42463821,\n",
              "                            42463860, 42463862]),\n",
              "       col_indices=tensor([ 56355, 137379, 194140,  ..., 571433, 135589,\n",
              "                           173752]),\n",
              "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]), device='cuda:0',\n",
              "       size=(576289, 576289), nnz=42463862, layout=torch.sparse_csr)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.adj_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'set_diag'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m adj_t \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49madj_t\u001b[39m.\u001b[39;49mto_sparse()\u001b[39m.\u001b[39;49mset_diag()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m deg \u001b[39m=\u001b[39m adj_t\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m deg_inv_sqrt \u001b[39m=\u001b[39m deg\u001b[39m.\u001b[39mpow(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'set_diag'"
          ]
        }
      ],
      "source": [
        "adj_t = data.adj_t.to_sparse().set_diag()\n",
        "deg = adj_t.sum(dim=1).to(torch.float)\n",
        "deg_inv_sqrt = deg.pow(-0.5)\n",
        "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
        "data.adj_t = adj_t\n",
        "\n",
        "predictor = LinkPredictor(args.hidden_channels, args.hidden_channels, 1,\n",
        "                        args.num_layers, args.dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "\n",
        "from ogb.linkproppred import Evaluator\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
        "        self.convs.append(\n",
        "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, adj_t)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def train(model, predictor, data, split_edge, optimizer, batch_size):\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(data.x.device)\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n",
        "                           shuffle=True):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h = model(data.x, data.adj_t)\n",
        "\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
        "\n",
        "        # Just do some trivial random sampling.\n",
        "        edge = torch.randint(0, data.num_nodes, edge.size(), dtype=torch.long,\n",
        "                             device=h.device)\n",
        "\n",
        "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
        "\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_examples = pos_out.size(0)\n",
        "        total_loss += loss.item() * num_examples\n",
        "        total_examples += num_examples\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, data, split_edge, evaluator, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    h = model(data.x, data.adj_t)\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(h.device)\n",
        "    pos_valid_edge = split_edge['valid']['edge'].to(h.device)\n",
        "    neg_valid_edge = split_edge['valid']['edge_neg'].to(h.device)\n",
        "    pos_test_edge = split_edge['test']['edge'].to(h.device)\n",
        "    neg_test_edge = split_edge['test']['edge_neg'].to(h.device)\n",
        "\n",
        "    pos_train_preds = []\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
        "\n",
        "    pos_valid_preds = []\n",
        "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
        "        edge = pos_valid_edge[perm].t()\n",
        "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
        "\n",
        "    neg_valid_preds = []\n",
        "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
        "        edge = neg_valid_edge[perm].t()\n",
        "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "    pos_test_preds = []\n",
        "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
        "        edge = pos_test_edge[perm].t()\n",
        "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
        "\n",
        "    neg_test_preds = []\n",
        "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
        "        edge = neg_test_edge[perm].t()\n",
        "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "    results = {}\n",
        "    for K in [10, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        train_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_train_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        valid_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_valid_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        test_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_test_pred,\n",
        "            'y_pred_neg': neg_test_pred,\n",
        "        })[f'hits@{K}']\n",
        "\n",
        "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GCN(in_channels=data.num_features, hidden_channels=256, out_channels=256, num_layers=3, dropout=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in train_loader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 46], edge_attr=[46, 3], x=[21, 9], y=[1, 1], num_nodes=21)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XnEW2NS6m8u"
      },
      "source": [
        "## SBM_PATTERN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCxG-CBR6m8w",
        "outputId": "c6de32f9-e966-4795-f93c-811e038c5c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I] Loading dataset SBM_PATTERN...\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dgl.graph'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m DATASET_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSBM_PATTERN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m dataset \u001b[39m=\u001b[39m LoadData(DATASET_NAME)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m trainset, valset, testset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtrain, dataset\u001b[39m.\u001b[39mval, dataset\u001b[39m.\u001b[39mtest\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/data.py:35\u001b[0m, in \u001b[0;36mLoadData\u001b[0;34m(DATASET_NAME)\u001b[0m\n\u001b[1;32m     33\u001b[0m SBM_DATASETS \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSBM_CLUSTER\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSBM_PATTERN\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39min\u001b[39;00m SBM_DATASETS: \n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m SBMsDataset(DATASET_NAME)\n\u001b[1;32m     37\u001b[0m \u001b[39m# handling for TSP dataset\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTSP\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/SBMs.py:160\u001b[0m, in \u001b[0;36mSBMsDataset.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    158\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/SBMs/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_dir\u001b[39m+\u001b[39mname\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 160\u001b[0m     f \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m f[\u001b[39m0\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval \u001b[39m=\u001b[39m f[\u001b[39m1\u001b[39m]\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dgl.graph'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "class DotDict(dict):\n",
        "    def __init__(self, **kwds):\n",
        "        self.update(kwds)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "\n",
        "DATASET_NAME = 'SBM_PATTERN'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJiZennk6m8y",
        "outputId": "51da0321-257c-45ba-9a6f-6b470c1e0b93"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp= dgrp.create_group('features/nodes')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['node_labels'] = np.array(l, dtype=int)\n",
        "\n",
        "\n",
        "dest_file = r'../datasets/SBM_PATTERN/SBM_PATTERN.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('SBM_PATTERN')\n",
        "    ds.attrs['max_node_feat'] = 2\n",
        "    ds.attrs['min_node_feat'] = 0\n",
        "    ds.attrs['num_min_nodes'] = 44\n",
        "    ds.attrs['num_max_nodes'] = 188\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, trainset)\n",
        "    save_dataset(val_ds, valset)\n",
        "    save_dataset(test_ds, testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-O7baZGNprD"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzGgzL8rqXuz"
      },
      "source": [
        "## SBM_CLUSTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdG8zDFtqXu1",
        "outputId": "55fc86af-3fd8-4351-87f4-c2de2cada5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I] Loading dataset SBM_CLUSTER...\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dgl.graph'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m DATASET_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSBM_CLUSTER\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m dataset \u001b[39m=\u001b[39m LoadData(DATASET_NAME)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m trainset, valset, testset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtrain, dataset\u001b[39m.\u001b[39mval, dataset\u001b[39m.\u001b[39mtest\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/data.py:35\u001b[0m, in \u001b[0;36mLoadData\u001b[0;34m(DATASET_NAME)\u001b[0m\n\u001b[1;32m     33\u001b[0m SBM_DATASETS \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSBM_CLUSTER\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSBM_PATTERN\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39min\u001b[39;00m SBM_DATASETS: \n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m SBMsDataset(DATASET_NAME)\n\u001b[1;32m     37\u001b[0m \u001b[39m# handling for TSP dataset\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTSP\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/SBMs.py:160\u001b[0m, in \u001b[0;36mSBMsDataset.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    158\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/SBMs/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_dir\u001b[39m+\u001b[39mname\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 160\u001b[0m     f \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m f[\u001b[39m0\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval \u001b[39m=\u001b[39m f[\u001b[39m1\u001b[39m]\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dgl.graph'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "class DotDict(dict):\n",
        "    def __init__(self, **kwds):\n",
        "        self.update(kwds)\n",
        "        self.__dict__ = self\n",
        "\n",
        "DATASET_NAME = 'SBM_CLUSTER'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yeIrgQLqXu4",
        "outputId": "93c57ce8-dfa8-41f0-85d4-dcdaa9e1879b"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp= dgrp.create_group('features/nodes')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['node_labels'] = np.array(l, dtype=int)\n",
        "\n",
        "dest_file = r'../datasets/SBM_CLUSTER/SBM_CLUSTER.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('SBM_CLUSTER')\n",
        "    ds.attrs['max_node_feat'] = 6\n",
        "    ds.attrs['min_node_feat'] = 0\n",
        "    ds.attrs['num_node_classes'] = 6\n",
        "    ds.attrs['num_min_nodes'] = 41\n",
        "    ds.attrs['num_max_nodes'] = 190\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, trainset)\n",
        "    save_dataset(val_ds, valset)\n",
        "    save_dataset(test_ds, testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ToKuw1Ns4c"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb3iNPVcljOU"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPye9cNCljOb",
        "outputId": "8f9d6270-af21-447a-88eb-330239b0e9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I] Loading dataset MNIST...\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dgl.graph'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/ml4ig1/Documents code/egt/create_hdf_benchmarking_datasets.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m LoadData\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m DATASET_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMNIST\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m dataset \u001b[39m=\u001b[39m LoadData(DATASET_NAME)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/egt/create_hdf_benchmarking_datasets.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m trainset, valset, testset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtrain, dataset\u001b[39m.\u001b[39mval, dataset\u001b[39m.\u001b[39mtest\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/data.py:21\u001b[0m, in \u001b[0;36mLoadData\u001b[0;34m(DATASET_NAME)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# handling for MNIST or CIFAR Superpixels\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMNIST\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCIFAR10\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m SuperPixDataset(DATASET_NAME)\n\u001b[1;32m     23\u001b[0m \u001b[39m# handling for (ZINC) molecule dataset\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mZINC\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m DATASET_NAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mZINC-full\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/Documents code/egt/benchmarking-gnns/data/superpixels.py:270\u001b[0m, in \u001b[0;36mSuperPixDataset.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    268\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/superpixels/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_dir\u001b[39m+\u001b[39mname\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 270\u001b[0m     f \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m f[\u001b[39m0\u001b[39m]\n\u001b[1;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval \u001b[39m=\u001b[39m f[\u001b[39m1\u001b[39m]\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dgl.graph'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "DATASET_NAME = 'MNIST'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR0ZYVY4ljOc",
        "outputId": "7987da11-f26e-42e2-f52a-66286e0aab7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp,defgrp = dgrp.create_group('features/nodes'), dgrp.create_group('features/edges')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        for fname, fval in g.edata.items():\n",
        "            defgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['label'] = l.numpy()\n",
        "\n",
        "\n",
        "dest_file = r'../datasets/MNIST/MNIST.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('MNIST')\n",
        "    ds.attrs['num_node_feat'] = 3\n",
        "    ds.attrs['num_edge_feat'] = 1\n",
        "    ds.attrs['num_classes'] = 10\n",
        "    ds.attrs['num_min_nodes'] = 40\n",
        "    ds.attrs['num_max_nodes'] = 75\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, dataset.train)\n",
        "    save_dataset(val_ds, dataset.val)\n",
        "    save_dataset(test_ds, dataset.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJjK6EBdNvIN"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZaIBGpmgkFU"
      },
      "source": [
        "## CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggsOg4mGgkFX",
        "outputId": "8db94b47-3da8-4aba-ede8-9c6c70fac578"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "DATASET_NAME = 'CIFAR10'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_7q1RBDgkFY",
        "outputId": "23305c1d-b229-48be-a748-935dd367ff3f"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp,defgrp = dgrp.create_group('features/nodes'), dgrp.create_group('features/edges')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        for fname, fval in g.edata.items():\n",
        "            defgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['label'] = l.numpy()\n",
        "\n",
        "\n",
        "dest_file = r'../datasets/CIFAR10/CIFAR10.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('CIFAR10')\n",
        "    ds.attrs['num_node_feat'] = 5\n",
        "    ds.attrs['num_edge_feat'] = 1\n",
        "    ds.attrs['num_classes'] = 10\n",
        "    ds.attrs['num_min_nodes'] = 85\n",
        "    ds.attrs['num_max_nodes'] = 150\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, dataset.train)\n",
        "    save_dataset(val_ds, dataset.val)\n",
        "    save_dataset(test_ds, dataset.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbIyYCQ_Nyl8"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02PQ5Hu9p7mG"
      },
      "source": [
        "## TSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K-xhLlYp7mI",
        "outputId": "c2a74d48-ef82-471e-ddc5-feaedffd2d2c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "DATASET_NAME = 'TSP'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB0D7T2p7mJ",
        "outputId": "0372993c-4828-4e67-bb1b-10e943712d97"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp,defgrp = dgrp.create_group('features/nodes'), dgrp.create_group('features/edges')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        for fname, fval in g.edata.items():\n",
        "            defgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['edge_labels'] = np.array(l, dtype=int)\n",
        "\n",
        "dest_file = r'../datasets/TSP/TSP.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('TSP')\n",
        "    ds.attrs['num_node_feat'] = 2\n",
        "    ds.attrs['num_edge_feat'] = 1\n",
        "    ds.attrs['num_min_nodes'] = 50\n",
        "    ds.attrs['num_max_nodes'] = 499\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, dataset.train)\n",
        "    save_dataset(val_ds, dataset.val)\n",
        "    save_dataset(test_ds, dataset.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_giG_aLKN3JE"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZpEXt-MYgPT"
      },
      "source": [
        "## ZINC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc-01cHRYie6",
        "outputId": "3c55281c-34dc-4366-ea2d-f66c125184ab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "DATASET_NAME = 'ZINC'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYoPyMFdTENJ",
        "outputId": "3d14c8be-5d04-41f2-d021-63dcfa525e44"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in tqdm(enumerate(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp,defgrp = dgrp.create_group('features/nodes'), dgrp.create_group('features/edges')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        for fname, fval in g.edata.items():\n",
        "            defgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['value'] = l.numpy()\n",
        "\n",
        "dest_file = r'../datasets/ZINC/ZINC.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('ZINC')\n",
        "    ds.attrs['num_atom_type'] = 28\n",
        "    ds.attrs['num_bond_type'] = 4\n",
        "    ds.attrs['num_min_atoms'] = 9\n",
        "    ds.attrs['num_max_atoms'] = 37\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, dataset.train)\n",
        "    save_dataset(val_ds, dataset.val)\n",
        "    save_dataset(test_ds, dataset.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz4hYr7KN548"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JNDwF24YmGa"
      },
      "source": [
        "## ZINC-full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vECgVD9AYmGf",
        "outputId": "fa972ca2-c44f-48d5-92d4-77302bea485d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from data.data import LoadData\n",
        "\n",
        "DATASET_NAME = 'ZINC-full'\n",
        "dataset = LoadData(DATASET_NAME)\n",
        "trainset, valset, testset = dataset.train, dataset.val, dataset.test #154s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGrNlpzmYmGf",
        "outputId": "4747c1d6-69aa-4ca7-8152-bd4d18d07cc8"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds,data):\n",
        "    for i, (g,l) in enumerate(tqdm(data)):\n",
        "        grp = ds.create_group(f'{i:0>10d}')\n",
        "        dgrp = grp.create_group('data')\n",
        "        \n",
        "        dgrp.attrs['num_nodes'] = g.number_of_nodes()\n",
        "        dgrp.attrs['num_edges'] = g.number_of_edges()\n",
        "        \n",
        "        dgrp['edges'] = torch.stack(g.edges(),axis=-1).numpy()\n",
        "        \n",
        "        dnfgrp,defgrp = dgrp.create_group('features/nodes'), dgrp.create_group('features/edges')\n",
        "        for fname, fval in g.ndata.items():\n",
        "            dnfgrp[fname] = fval.numpy()\n",
        "        for fname, fval in g.edata.items():\n",
        "            defgrp[fname] = fval.numpy()\n",
        "        \n",
        "        tgrp = grp.create_group('targets')\n",
        "        tgrp['value'] = l.numpy()\n",
        "\n",
        "dest_file = r'../datasets/ZINC_full/ZINC_full.h5'\n",
        "Path(dest_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "with h5py.File(dest_file, 'w') as file:\n",
        "    ds = file.create_group('ZINC_full')\n",
        "    ds.attrs['num_atom_type'] = 28\n",
        "    ds.attrs['num_bond_type'] = 4\n",
        "    ds.attrs['num_min_atoms'] = 6\n",
        "    ds.attrs['num_max_atoms'] = 38\n",
        "    train_ds, val_ds, test_ds = ds.create_group('training'), ds.create_group('validation'), ds.create_group('test')\n",
        "    save_dataset(train_ds, dataset.train)\n",
        "    save_dataset(val_ds, dataset.val)\n",
        "    save_dataset(test_ds, dataset.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slnYEKKYN9UU"
      },
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CztP0sgkGseB"
      },
      "source": [
        "# Create a Parent HDF with External Soft-links (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooOnr8Z5GrOc",
        "outputId": "8dd2a87b-df0f-40cc-ac36-18a8e79b8f90"
      },
      "outputs": [],
      "source": [
        "%cd ../datasets/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_fyKPtRHA57"
      },
      "outputs": [],
      "source": [
        "import h5py as h5\n",
        "\n",
        "def add_link(data_file, dataset_name):\n",
        "    data_file[f'{dataset_name}'] = h5.ExternalLink(f\"{dataset_name}/{dataset_name}.h5\", f\"/{dataset_name}\")\n",
        "\n",
        "\n",
        "with h5.File(\"gnn_benchmark.h5\", 'w') as data_file:\n",
        "    add_link(data_file,\"SBM_PATTERN\")\n",
        "    add_link(data_file,\"SBM_CLUSTER\")\n",
        "    add_link(data_file,\"MNIST\")\n",
        "    add_link(data_file,\"CIFAR10\")\n",
        "    add_link(data_file,\"TSP\")\n",
        "    add_link(data_file,\"ZINC\")\n",
        "    add_link(data_file,\"ZINC_full\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVw9QvgtJVxi"
      },
      "source": [
        "## Test the File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ujrGH9JUq8",
        "outputId": "9a1509bc-f117-44bb-8e72-afc154da4770"
      },
      "outputs": [],
      "source": [
        "with h5.File(\"gnn_benchmark.h5\", 'r') as data_file:\n",
        "    datasets = list(data_file.keys())\n",
        "    print(datasets)\n",
        "    for dset in datasets:\n",
        "        print()\n",
        "        print(dset)\n",
        "        print('------------')\n",
        "        print(f'Training examples  : {len(data_file[dset][\"training\"])}')\n",
        "        print(f'Validation examples: {len(data_file[dset][\"validation\"])}')\n",
        "        print(f'Test examples      : {len(data_file[dset][\"test\"])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw_UfnjWK1q8"
      },
      "source": [
        "# Compress Files for Storage, Download etc. (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBpl2PJGK02M",
        "outputId": "ed590e6d-4903-442e-b5ee-fe7bf5e67b49"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyNH3vXXL15D"
      },
      "outputs": [],
      "source": [
        "!tar -czf datasets.tar.gz datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npYXEaInOBlk",
        "outputId": "d0564d68-8074-49b7-cc8c-5aaa71da5db2"
      },
      "outputs": [],
      "source": [
        "%ls -alh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PNy78vRkdB"
      },
      "source": [
        "## (On Google Colab) Download File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8N_IznFhRhV0",
        "outputId": "7457b26b-b5fe-4d14-961c-c8d19979a2f0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('datasets.tar.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai1GUxtdSboV"
      },
      "source": [
        "## (On Google Colab) Or Transfer to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hdYLs0RSbMW",
        "outputId": "3655fa1c-f0fc-477b-af16-de620adcee5a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')\n",
        "!cp -v datasets.tar.gz ./drive/MyDrive/\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0g-JTMJcMHzg",
        "DrzCtDPZRm7w",
        "9XnEW2NS6m8u",
        "VzGgzL8rqXuz",
        "Tb3iNPVcljOU",
        "iZaIBGpmgkFU",
        "02PQ5Hu9p7mG",
        "PZpEXt-MYgPT",
        "6JNDwF24YmGa",
        "CztP0sgkGseB",
        "ai1GUxtdSboV"
      ],
      "name": "create_hdf_datasets.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
